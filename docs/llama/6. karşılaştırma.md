# Llama Modellerini Karşılaştırma

**Güncelleme: Llama 3, 18 Nisan'da yayınlandı ve bu not defteri, Together.ai tarafından barındırılan Llama 3 ve Llama 2 modellerini karşılaştırmak için güncellendi.**

Llama modellerini yönlendirmek için yardımcı fonksiyonu yükle:

```python
from utils import llama, llama_chat
```

**Görev 1: Duygu Sınıflandırma** 
Modelleri birkaç-atış komut duygu sınıflandırmasında karşılaştırın. Modelden tek kelime ile yanıt vermesini istiyorsunuz.

```python
prompt = '''
Mesaj: Merhaba Amit, düşünceli doğum günü kartın için teşekkürler!
Duygu: Pozitif
Mesaj: Merhaba Baba, piyano resitalime 20 dakika geç kaldın!
Duygu: Negatif
Mesaj: Bu akşam pizza sipariş etmek için sabırsızlanıyorum!
Duygu: ?
Tek kelime ile yanıt ver.
'''
```

İlk olarak, 7B parametreli sohbet modeli (llama-2-7b-chat) ile yanıt alın. Together.ai tarafından kabul edilen model isimleri büyük/küçük harfe duyarlı değildir ve "META-LLAMA/LLAMA-2-7B-CHAT-HF" veya "togethercomputer/llama-2-7b-chat" olabilir. Şimdi "META-LLAMA" ile başlayan isimler tercih ediliyor.

```python
response = llama(prompt, model="META-LLAMA/Llama-2-7B-CHAT-HF")
print(response)
```

Şimdi, aynı görevde 70B parametreli sohbet modeli (llama-2-70b-chat) kullanın

```python
response = llama(prompt, model="META-LLAMA/Llama-2-70B-CHAT-HF")
print(response)
```

Llama 3 sohbet modellerini kullanma

```python
response = llama(prompt, model="META-LLAMA/Llama-3-8B-CHAT-HF")
print(response)
response = llama(prompt, model="META-LLAMA/Llama-3-70B-CHAT-HF")
print(response)
```

**Görev 2: Özetleme** 
Modelleri özetleme görevinde karşılaştırın. Bu, kursun önceki bölümlerinde kullandığınız aynı "email".

```python
email = """
Sevgili Amit,

Büyük dil modelleri (LLMs) artık açık kaynaklı veya neredeyse açık kaynaklı. Nispeten serbest lisanslara sahip modellerin çoğalması, geliştiricilere uygulamalar oluşturmak için daha fazla seçenek sunuyor.

İşte LLM'leri kullanarak uygulamalar oluşturmanın bazı farklı yolları, maliyet/karmaşıklık sırasına göre artarak:

Talimatlandırma. Önceden eğitilmiş bir LLM'ye talimat vermek, eğitim seti olmadan dakikalar veya saatler içinde bir prototip oluşturmanızı sağlar. Bu yılın başlarında, birçok kişinin talimatlandırmayla denemeler yapmaya başladığını gördüm ve bu ivme kesintisiz devam ediyor. Kısa kurslarımızdan birkaçı, bu yaklaşım için en iyi uygulamaları öğretiyor.

Tek atış veya birkaç atış talimatlandırma. Bir komutun yanı sıra, LLM'ye bir görevi nasıl yapacağının birkaç örneğini vermek — giriş ve istenen çıktı — bazen daha iyi sonuçlar verir.

İnce ayar. Çok miktarda metin üzerinde önceden eğitilmiş bir LLM, kendi küçük veri setiniz üzerinde daha fazla eğitilerek görev

inize ince ayar yapılabilir. İnce ayar araçları olgunlaşıyor, bu da onları daha fazla geliştirici için erişilebilir hale getiriyor.

Ön eğitim. Kendi LLM'nizi sıfırdan ön eğitmek çok kaynak gerektirir, bu yüzden çok az takım bunu yapar. Çeşitli konularda önceden eğitilmiş genel amaçlı modellere ek olarak, bu yaklaşım finans hakkında bilgi sahibi olan BloombergGPT gibi ve tıbba odaklanan Med-PaLM 2 gibi özelleşmiş modellerin ortaya çıkmasına yol açmıştır.

Çoğu takım için, uygulamanızı hızla çalışır hale getirmenizi sağladığı için talimatlandırmayla başlamanızı öneririm. Çıktı kalitesinden memnun değilseniz, daha karmaşık tekniklere yavaş yavaş geçin. Birkaç örnek ile tek atış veya birkaç atış talimatlandırmaya başlayın. Bu yeterince iyi çalışmazsa, belki de LLM'nin yüksek kaliteli çıktılar üretmek için gereken ana bilgilerle komutları daha da iyileştirmek için RAG (bilgi artırılmış üretim) kullanın. Bu hala istediğiniz performansı sağlamazsa, o zaman ince ayarı deneyin — ancak bu, önemli ölçüde daha fazla karmaşıklık düzeyini temsil eder ve yüzlerce veya binlerce örnek daha gerektirebilir. Bu seçenekleri derinlemesine anlamak için, AWS ve DeepLearning.AI tarafından oluşturulan Büyük Dil Modelleri ile Yaratıcı AI kursunu şiddetle tavsiye ederim.

(İlginç bir gerçek: DeepLearning.AI ekibinden bir üye, Llama-2-7B'yi benim gibi ses çıkarması için ince ayar yapmaya çalışıyor. Acaba işim tehlikede mi? 😜)

Tescilli bir model olan ve ince ayara açık olmayan GPT-4 gibi bir modelden sonra ince ayara geçmek istiyorsanız ek karmaşıklık ortaya çıkar. Çok daha küçük bir modeli ince ayarlamak, daha büyük, daha yetenekli bir modeli talimatlandırmaktan daha üstün sonuçlar sağlama olasılığı var mı? Cevap genellikle uygulamanıza bağlıdır. Bir LLM'nin çıktı stilini değiştirmek amacınızsa, daha küçük bir modeli ince ayarlamak iyi işleyebilir. Ancak, uygulamanız GPT-4'ü karmaşık akıl yürütme yapması için talimatlandırıyorsa — GPT-4 mevcut açık modelleri aştığı durumlarda — daha küçük bir modeli üstün sonuçlar verecek şekilde ince ayarlamak zor olabilir.

Bir geliştirme yaklaşımı seçmenin ötesinde, belirli bir model seçmek de gereklidir. Daha küçük modeller daha az işlem gücü gerektirir ve birçok uygulama için iyi çalışır, ancak daha büyük modeller genellikle dünya hakkında daha fazla bilgiye sahiptir ve daha iyi akıl yürütme yeteneğine sahiptir. Bu seçimi nasıl yapacağınızı ilerideki bir mektupta anlatacağım.

Öğrenmeye devam edin!

Andrew
"""

İlk olarak, 7B parametreli sohbet modeli (llama-2-7b-chat) ile e-postayı özetleyin.

```python
response_7b = llama(prompt, model="META-LLAMA/Llama-2-7B-CHAT-HF")
print(response_7b)
```

Şimdi, 13B parametreli sohbet modeli (llama-2-13

b-chat) ile e-postayı özetleyin.

```python
response_13b = llama(prompt, model="META-LLAMA/Llama-2-13B-CHAT-HF")
print(response_13b)
```

Son olarak, 70B parametreli sohbet modeli (llama-2-70b-chat) ile e-postayı özetleyin.

```python
response_70b = llama(prompt, model="META-LLAMA/Llama-2-70B-CHAT-HF")
print(response_70b)
```

Llama 3 sohbet modellerini kullanma

```python
response_llama3_8b = llama(prompt, model="META-LLAMA/Llama-3-8B-CHAT-HF")
print(response_llama3_8b)
response_llama3_70b = llama(prompt, model="META-LLAMA/Llama-3-70B-CHAT-HF")
print(response_llama3_70b)
```
